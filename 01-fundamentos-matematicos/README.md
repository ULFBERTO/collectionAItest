# ğŸ“ Fundamentos MatemÃ¡ticos para IA

No necesitas recordar todas las ecuaciones de memoria. Lo importante es **entender los conceptos** y saber aplicarlos con cÃ³digo.

## ğŸ¯ Objetivos

Al finalizar esta secciÃ³n, comprenderÃ¡s:
- âœ… Operaciones con vectores y matrices
- âœ… Por quÃ© son importantes en IA
- âœ… Gradientes y derivadas (sin ecuaciones complicadas)
- âœ… CÃ³mo se usan en el entrenamiento de modelos

## ğŸ“š Contenido

### 1. Ãlgebra Lineal Simplificada

#### ğŸ”¢ Vectores
Un vector es simplemente una lista de nÃºmeros:

```python
# En Python con NumPy
import numpy as np

vector = np.array([1, 2, 3, 4])
print(f"Vector: {vector}")
```

**Â¿Por quÃ© son importantes?**
- Cada imagen es un vector de pÃ­xeles
- Cada palabra se puede representar como un vector
- Los modelos de IA procesan vectores

#### ğŸ“Š Matrices
Una matriz es una tabla de nÃºmeros:

```python
matriz = np.array([
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
])
print(f"Matriz:\n{matriz}")
```

**Â¿Por quÃ© son importantes?**
- Los pesos de una red neuronal son matrices
- Las transformaciones de datos usan matrices

#### âš¡ Operaciones BÃ¡sicas

```python
# Suma de vectores
v1 = np.array([1, 2, 3])
v2 = np.array([4, 5, 6])
suma = v1 + v2  # [5, 7, 9]

# Producto punto (dot product)
producto = np.dot(v1, v2)  # 1*4 + 2*5 + 3*6 = 32

# MultiplicaciÃ³n de matrices
A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
C = np.dot(A, B)
```

### 2. CÃ¡lculo para IA (Sin Miedo)

#### ğŸ“‰ Derivadas - Â¿QuÃ© son?
La derivada te dice **cuÃ¡nto cambia algo**.

**Ejemplo intuitivo:**
- Si tienes un error de 10 y despuÃ©s de 8
- La derivada te dice que **estÃ¡s mejorando** (-2)

```python
# Ejemplo prÃ¡ctico: funciÃ³n cuadrÃ¡tica
def funcion(x):
    return x**2

# La derivada de x^2 es 2*x
def derivada(x):
    return 2*x

# Si estÃ¡s en x=3, la derivada es 6
# Esto significa: si aumentas x un poco, y aumenta 6 veces mÃ¡s
x = 3
print(f"En x={x}, la funciÃ³n vale {funcion(x)}")
print(f"La derivada (tasa de cambio) es {derivada(x)}")
```

#### ğŸ¯ Gradiente Descendente
Es la tÃ©cnica para **minimizar el error** de un modelo.

**AnalogÃ­a:** Imagina que estÃ¡s en una montaÃ±a con niebla y quieres bajar:
1. Miras a tu alrededor (calculas el gradiente)
2. Te mueves hacia donde baja mÃ¡s (sigues el gradiente)
3. Repites hasta llegar abajo (mÃ­nimo error)

```python
# Ejemplo simple de gradiente descendente
def error(w):
    # Error cuadrÃ¡tico simple
    return (w - 5)**2

def derivada_error(w):
    # Derivada del error respecto a w
    return 2 * (w - 5)

# Proceso de optimizaciÃ³n
w = 0  # Valor inicial
learning_rate = 0.1  # TamaÃ±o del paso

for i in range(10):
    gradiente = derivada_error(w)
    w = w - learning_rate * gradiente  # ActualizaciÃ³n
    print(f"IteraciÃ³n {i+1}: w={w:.2f}, error={error(w):.2f}")

# w converge a 5 (donde el error es mÃ­nimo)
```

### 3. Probabilidad BÃ¡sica

#### ğŸ² Conceptos Clave

```python
import numpy as np

# SimulaciÃ³n de lanzamientos de moneda
lanzamientos = np.random.choice(['cara', 'cruz'], size=1000)
probabilidad_cara = np.sum(lanzamientos == 'cara') / 1000
print(f"Probabilidad de cara: {probabilidad_cara:.2f}")

# DistribuciÃ³n normal (muy usada en IA)
datos = np.random.normal(loc=0, scale=1, size=1000)
# loc = media, scale = desviaciÃ³n estÃ¡ndar
```

## ğŸ‹ï¸ Ejercicios PrÃ¡cticos

### Ejercicio 1: Operaciones con Vectores
```python
# TODO: Implementa estas funciones
def magnitud_vector(v):
    """Calcula la magnitud (longitud) de un vector"""
    pass

def similitud_coseno(v1, v2):
    """Calcula quÃ© tan similares son dos vectores (0 a 1)"""
    pass
```

### Ejercicio 2: Gradiente Descendente
```python
# TODO: Implementa gradiente descendente para una funciÃ³n lineal
def gradiente_descendente_lineal(X, y, epochs=100, lr=0.01):
    """
    Encuentra la mejor lÃ­nea que se ajusta a los datos
    X: datos de entrada
    y: valores objetivo
    epochs: nÃºmero de iteraciones
    lr: learning rate
    """
    pass
```

## ğŸ“– Recursos Adicionales

- **Cheat Sheet**: Ver [`algebra-lineal-cheatsheet.md`](./algebra-lineal-cheatsheet.md)
- **Visualizaciones**: Ver [`visualizaciones.ipynb`](./visualizaciones.ipynb)
- **Soluciones**: Ver [`soluciones/`](./soluciones/)

## âœ… AutoevaluaciÃ³n

Antes de continuar a la siguiente fase, asegÃºrate de poder:

- [ ] Crear y manipular vectores y matrices en NumPy
- [ ] Entender quÃ© es un producto punto y por quÃ© es Ãºtil
- [ ] Explicar quÃ© es una derivada con tus propias palabras
- [ ] Implementar gradiente descendente simple
- [ ] Generar nÃºmeros aleatorios con distribuciones

---

**Siguiente:** [Fase 2 - Python para IA](../02-python-para-ia/README.md)
